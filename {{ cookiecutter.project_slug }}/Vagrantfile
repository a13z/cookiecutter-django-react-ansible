# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.require_version ">= 2.2.5"

if Vagrant::Util::Platform.windows? then
  def running_as_admin?
    # query the LOCAL SERVICE account reg key (requires admin privileges)
    system('reg query "HKU\S-1-5-19"', :out => File::NULL)
  end

  unless running_as_admin?
    puts "Admin rights are required to create symlinks. Try running this Vagrantfile again from an admin command prompt."
    exit 1
  end
end

Vagrant.configure("2") do |config|

  config.vagrant.plugins = {
    "vagrant-proxyconf" => {"version" => ">=2.0.10"},
    "vagrant-vbguest" => {"version" => ">=0.24.0"},
    "vagrant-hostmanager" => {"version" => ">=1.8.9"}
  }

  config.vm.box = "ubuntu/focal64"
  # disable the default vagrant mount. We configure custom mounts on the VM separately.
  config.vm.synced_folder ".", "/vagrant", disabled: true

  # There seems to be a VirtualBox bug where the VirtualBox processes on the host machine
  # can get stuck at 100% after resuming from sleep on macOS hosts, even when the guest
  # machines are idle. We disable audio devices on all VMs as a workaround.
  # See https://www.virtualbox.org/ticket/18089
  config.vm.provider "virtualbox" do |virtualbox|
    virtualbox.customize ["modifyvm", :id, "--audio", "none"]
  end

  config.apt_proxy.http = "{{ cookiecutter.vagrant_apt_proxy }}"
  config.proxy.http = "{{ cookiecutter.vagrant_http_proxy }}"
  config.proxy.https = "{{ cookiecutter.vagrant_https_proxy }}"

  config.vbguest.auto_update = true

  config.hostmanager.enabled = true
  config.hostmanager.manage_host = true
  config.hostmanager.manage_guest = true

  config.vm.define "dev", primary: true do |machine|
    machine.vm.hostname = "{{ cookiecutter.frontend_app_dev_hostname }}"
    machine.hostmanager.aliases = ["{{ cookiecutter.backend_app_dev_hostname }}"]
    machine.vm.network "private_network", ip: "172.17.177.21"
    # Enables X11 forwarding. This can be useful when running Selenium tests
    # since you can see exactly what's happening in the browser during the tests.
    # This requires an X11 server on your host machine, see https://www.xquartz.org
    # for Mac hosts or http://x.cygwin.com for Windows hosts.
    machine.ssh.forward_x11 = true
    machine.vm.synced_folder(
      "./backend_app",
      "/opt/{{ cookiecutter.project_slug }}_backend/current",
      type: "virtualbox"
    )

    machine.vm.provider "virtualbox" do |virtualbox|
      virtualbox.name = "{{ cookiecutter.project_slug }}_dev"
      virtualbox.memory = 1024
      virtualbox.cpus = 1
    end
  end

  config.vm.define "db" do |machine|
    machine.vbguest.no_install = true
    machine.vm.hostname = "{{ cookiecutter.database_dev_hostname }}"
    machine.vm.network "private_network", ip: "172.17.177.22"
    machine.vm.provider "virtualbox" do |virtualbox|
      virtualbox.name = "{{ cookiecutter.project_slug }}_db"
      virtualbox.memory = 512
    end
  end

  # We spin up another VM and execute ansible using the ansible_local vagrant
  # provisioner, as opposed to executing ansible directly from the host machine.
  #
  # This helps simplify the initial setup process, creates a consistent
  # provisioning environment and allows us to provision a dev environment on
  # host machines where Ansible is not supported, such as Windows.
  # (see https://docs.ansible.com/ansible/latest/user_guide/windows_faq.html#can-ansible-run-on-windows )
  #
  # However, this process does result in the creation of a 'provisioner' VM,
  # which increases the system resources used for this dev environment.
  config.vm.define "provisioner" do |machine|
    # By default Vagrant adds the host's name to the loopback address when
    # setting machine.vm.hostname. This can result in unexpected behaviour
    # when attempting to bind to the host, such as when running the frontend
    # app dev server with 'yarn start'. So, we instead use a hostname alias and
    # set the hostname later on ourselves in an inline shell script.
    # See https://github.com/hashicorp/vagrant/issues/7263 for more.
    machine.hostmanager.aliases = ["{{ cookiecutter.provisioner_hostname }}"]
    machine.vm.network "private_network", ip: "172.17.177.20"
    machine.vm.synced_folder "./", "/vagrant", type: "virtualbox"
    # Mount the folder containing the dev vm ssh keys with explicit file
    # permissions. This addresses 'unprotected private key file' errors on
    # Windows, due to mounts on Windows having 777 file permissions by default.
    machine.vm.synced_folder(
      "./.vagrant/machines",
      "/vagrant_machines",
      type: "virtualbox",
      mount_options: ["fmode=600"]
    )

    machine.vm.provider "virtualbox" do |virtualbox|
      virtualbox.name = "{{ cookiecutter.project_slug }}_provisioner"
      virtualbox.memory = 1024
    end

    $provisioner_vm_setup_script = <<-SCRIPT
      sudo hostnamectl set-hostname {{ cookiecutter.provisioner_hostname }}
      # install ansible requirements
      sudo apt-get update
      sudo apt-get install -y python3 python3-venv python3-pip python3-apt sshpass libssl-dev
      python3.8 -m venv ~/provision_venv --system-site-packages
      source ~/provision_venv/bin/activate
      pip install pip==20.3.3 wheel==0.36.2
      pip install -r /vagrant/provisioning/requirements.txt
    SCRIPT

    machine.vm.provision "shell", inline: $provisioner_vm_setup_script, privileged: false, name: "ansible setup"

    # See https://www.vagrantup.com/docs/provisioning/ansible_local.html and
    # https://www.vagrantup.com/docs/provisioning/ansible_common.html
    machine.vm.provision "ansible_local" do |ansible|
      ansible.compatibility_mode = "2.0"
      ansible.playbook = "playbook.yml"
      ansible.playbook_command = "~/provision_venv/bin/ansible-playbook"
      ansible.provisioning_path = "/vagrant/provisioning"
      ansible.config_file = "ansible-vagrant.cfg"
      ansible.install = false # we do this in the above inline setup script
      ansible.galaxy_role_file = "requirements.yml"
      ansible.galaxy_command = " ~/provision_venv/bin/ansible-galaxy install --role-file=%{role_file}"
      ansible.verbose = false
      ansible.limit = "all,localhost"
      ansible.inventory_path = "environments/dev/inventory"
      ansible.tags = ENV["VAGRANT_ANSIBLE_TAGS"] || "all"
    end
    # Shut down the provisioner VM once it has finished provisioning the other VMs.
    # machine.vm.provision "shell", inline: "shutdown -h now"
  end

end
